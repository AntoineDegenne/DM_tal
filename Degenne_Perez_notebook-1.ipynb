{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import re\n",
    "import os.path\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.metrics import f1_score, confusion_matrix, plot_confusion_matrix, average_precision_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données:\n",
    "def load_pres(fname):\n",
    "    alltxts = []\n",
    "    alllabs = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        #\n",
    "        lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
    "        if lab.count('M') >0:\n",
    "            alllabs.append(-1)\n",
    "        else: \n",
    "            alllabs.append(1)\n",
    "        alltxts.append(txt)\n",
    "    return alltxts,alllabs\n",
    "\n",
    "fname = \"/Users/etienneperez/Documents/IODAA/TAL/ressources/AFDpresidentutf8/corpus.tache1.learn.utf8\"\n",
    "\n",
    "alltxts,alllabs = load_pres(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrase :  57413\n",
      "Nombre d'étiquettes 1 :  49890  ( 0.87 )\n",
      "Nombre d'étiquettes -1 :  7523  ( 0.13 )\n"
     ]
    }
   ],
   "source": [
    "# Nombre de phrase\n",
    "print(\"Nombre de phrase : \", len(alltxts))\n",
    "\n",
    "# Mise en évidénce du cas déséquilibré\n",
    "print(\"Nombre d'étiquettes 1 : \", alllabs.count(1), \" (\",round(alllabs.count(1)/len(alllabs),2),\")\")\n",
    "print(\"Nombre d'étiquettes -1 : \", alllabs.count(-1), \" (\",round(alllabs.count(-1)/len(alllabs),2),\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de prétraitements basiques\n",
    "\n",
    "# Supprimer la ponctuation et les chiffres de toutes les phrases\n",
    "def suppr_ponct(txt): # prend alltxts en entrée\n",
    "    punc = string.punctuation  \n",
    "    punc += '\\n\\r\\t'\n",
    "\n",
    "    for i in range(len(txt)):\n",
    "        txt[i] = txt[i].translate(str.maketrans(punc, ' ' * len(punc)))  \n",
    "        txt[i] = re.sub('[0-9]+', '', txt[i])\n",
    "\n",
    "# Supprimer les accents et caractères spéciaux\n",
    "def suppr_accents_maj(txt): # prend alltxts en entrée\n",
    "    for i in range(len(txt)):\n",
    "        txt[i] = unicodedata.normalize('NFD',  txt[i]).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "        txt[i] =  txt[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération du dictionnaire {\"mot\" : occurence_documentaire}\n",
    "suppr_ponct(alltxts)\n",
    "suppr_accents_maj(alltxts)\n",
    "\n",
    "words = []\n",
    "for i in range(len(alltxts)):\n",
    "    mots_vus = []\n",
    "    for mot in alltxts[i].split():\n",
    "        if mot not in mots_vus: # on ajoute 1 seule fois le mot s'il apparait dans la phrase --> occurence documentaire et pas totale\n",
    "            words.append(mot)\n",
    "            mots_vus.append(mot)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "dico = Counter(words)\n",
    "\n",
    "# Dico qui associe 1 mot à sa position dans le dictionnaire dico\n",
    "trans = dict(zip(list(dico.keys()), np.arange(len(dico)).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27054\n"
     ]
    }
   ],
   "source": [
    "# Extraction du vocabulaire\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(alltxts)\n",
    "\n",
    "print(\"Taille du vocabulaire : \",len(vectorizer.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de', 35268), ('la', 27664), ('et', 26298), ('l', 24658), ('a', 23599), ('le', 19625), ('les', 18229), ('d', 16258), ('des', 16088), ('est', 14927), ('en', 13349), ('que', 13140), ('qui', 12395), ('un', 10791), ('une', 10692), ('dans', 10113), ('pour', 9892), ('du', 8966), ('il', 8861), ('je', 8808), ('nous', 7696), ('au', 7269), ('c', 7248), ('vous', 7140), ('ce', 6990), ('plus', 5882), ('pas', 5704), ('qu', 5551), ('sur', 5223), ('s', 5094), ('notre', 5063), ('france', 4937), ('ne', 4890), ('par', 4849), ('cette', 4558), ('mais', 4388), ('avec', 4345), ('aussi', 4232), ('se', 4163), ('nos', 4138), ('pays', 4126), ('ou', 4122), ('sont', 4104), ('elle', 3848), ('n', 3759), ('ont', 3641), ('aux', 3615), ('etre', 3608), ('tout', 3453), ('tous', 3247), ('j', 3163), ('votre', 3059), ('leur', 2994), ('y', 2991), ('son', 2834), ('bien', 2821), ('nom', 2755), ('ces', 2712), ('meme', 2692), ('comme', 2642), ('hui', 2558), ('aujourd', 2557), ('on', 2497), ('entre', 2476), ('doit', 2389), ('europe', 2382), ('monde', 2365), ('faire', 2327), ('ses', 2322), ('sa', 2286), ('francais', 2282), ('ai', 2147), ('faut', 2015), ('ils', 2006), ('si', 1991), ('ete', 1974), ('sans', 1928), ('fait', 1898), ('etat', 1847), ('date', 1740), ('cela', 1695), ('dire', 1635), ('ensemble', 1628), ('avez', 1625), ('peut', 1582), ('deux', 1581), ('developpement', 1576), ('dont', 1565), ('tres', 1563), ('leurs', 1522), ('encore', 1480), ('politique', 1475), ('autres', 1471), ('president', 1466), ('monsieur', 1449), ('toutes', 1413), ('vie', 1389), ('avons', 1386), ('vos', 1344), ('ici', 1322)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f79b38cd340>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage des 100 mots les plus apparus dans les phrases\n",
    "print(\"100 mots les plus apparus : \",dico.most_common(100))\n",
    "freq = [f for w,f in dico.most_common()]\n",
    "\n",
    "plt.plot(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation paramétrique du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITION DES FONCTIONS DE PRETRAITEMENT\n",
    "\n",
    "# Data augmentation pour rééquilibrer les classes\n",
    "def data_augmentation(txt,labs):\n",
    "    liste_positions = []\n",
    "    for i in labs:\n",
    "        liste_positions.append(i == 1) \n",
    "\n",
    "    liste_classe_minoritaire = itertools.compress(txt, liste_positions)\n",
    "\n",
    "    txts_bis = []\n",
    "    for each in liste_classe_minoritaire:\n",
    "        txts_bis.append(each)\n",
    "\n",
    "    for elem in txts_bis: # nombre d'ajout trouvé de manière empirique pour avoir environ 50/50\n",
    "        txt.append(elem)\n",
    "        txt.append(elem)\n",
    "        txt.append(elem)\n",
    "        txt.append(elem)\n",
    "        txt.append(elem)\n",
    "        labs.append(1)\n",
    "        labs.append(1)\n",
    "        labs.append(1)\n",
    "        labs.append(1)\n",
    "        labs.append(1)\n",
    "\n",
    "# Stemming\n",
    "def stemming(txt):\n",
    "    ps = PorterStemmer()\n",
    "    compteur = 0\n",
    "    for sentence in txt:\n",
    "        # print(\"sentence : \",sentence)\n",
    "        words = word_tokenize(sentence,language='french')\n",
    "        # print(\"words : \",words)\n",
    "        sentence_new = ''\n",
    "        for word in words:\n",
    "            word_new = ps.stem(word)\n",
    "            sentence_new += word_new + ' '\n",
    "        txt[compteur] = sentence_new\n",
    "        compteur += 1\n",
    "\n",
    "# Stopword\n",
    "stop_words = stopwords.words('french')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISE EN PLACE DES PRETRAITEMENTS --> GENERATION DE DIFFERENTS JEUX DE DONNEES\n",
    "# Résumé : pour chaque type de traitement du cas déséquilibré (data augmentation ou pénalisation erreur classe min), on génère 3 jeux de données issus de\n",
    "# 3 prétraitrements différents : prétaitements basiques + stop words + stemming, n-gram niveau mots, n-gram niveau caractères\n",
    "\n",
    "# Séparation train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(alltxts, alllabs, test_size=0.3, random_state=0)\n",
    "\n",
    "# On inverse les étiquettes 1 et -1 pour que la classe minoritaire soit étiquetée +1\n",
    "# Cela permet de considérer la classe minoritaire comme la classe d'intérêt dans les métriques de scoring de gridSearchCV\n",
    "for i in range(0,len(y_train)):\n",
    "    if y_train[i] == -1:\n",
    "        y_train[i] = 1\n",
    "    else:\n",
    "        y_train[i] = -1\n",
    "\n",
    "for i in range(0,len(y_test_1)):\n",
    "    if y_test[i] == -1:\n",
    "        y_test[i] = 1\n",
    "    else:\n",
    "        y_test[i] = -1\n",
    "\n",
    "# On ajoute à liste_donnees différents jeux de données issus de différents pré-traitements\n",
    "liste_donnees = []\n",
    "\n",
    "# Prétraitement par stemming commun à tous les jeux de données\n",
    "stemming(X_train)\n",
    "stemming(X_test)\n",
    "\n",
    "#######################################\n",
    "# PRETRAITEMENTS SANS DATA AUGMENTATION\n",
    "#######################################\n",
    "X_train_1 = X_train.copy()\n",
    "X_test_1 = X_test.copy()\n",
    "y_train_1 = y_train.copy()\n",
    "y_test_1 = y_test.copy()\n",
    "\n",
    "# Prétraitement avec pré-traitements basiques + stop words + stemming\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, min_df = 2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_1)\n",
    "X_test_vec = vectorizer.transform(X_test_1)\n",
    "liste_donnees.append([X_train_vec,y_train_1,X_test_vec,y_test_1,len(vectorizer.get_feature_names()), \"Prétraitements basiques + stopwords + stemming : \"])\n",
    "\n",
    "# Prétraitement avec bigram et trigram au niv mot (+ pré-traitements basiques + stop words + stemming)\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, ngram_range=(1,3), analyzer='word', min_df = 2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_1)\n",
    "X_test_vec = vectorizer.transform(X_test_1)\n",
    "liste_donnees.append([X_train_vec,y_train_1,X_test_vec,y_test_1,len(vectorizer.get_feature_names()), \"Prétraitements basiques + stopwords + stemming + bigram/trigram niveau mot : \"])\n",
    "\n",
    "# Prétraitement avec bigram et trigram au niv caractère (+ pré-traitements basiques + stop words + stemming)\n",
    "vectorizer = CountVectorizer(stop_words = stop_words,ngram_range=(2,4), analyzer='char', min_df = 2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_1)\n",
    "X_test_vec = vectorizer.transform(X_test_1)\n",
    "liste_donnees.append([X_train_vec,y_train_1,X_test_vec,y_test_1,len(vectorizer.get_feature_names()), \"Prétraitements basiques + stopwords + stemming + bigram/trigram niveau caractère: \"])\n",
    "\n",
    "#######################################\n",
    "# PRETRAITEMENTS AVEC DATA AUGMENTATION\n",
    "#######################################\n",
    "X_train_2 = X_train.copy()\n",
    "X_test_2 = X_test.copy()\n",
    "y_train_2 = y_train.copy()\n",
    "y_test_2 = y_test.copy()\n",
    "data_augmentation(X_train_2,y_train_2)\n",
    "\n",
    "# Prétraitement avec pré-traitements basiques + stop words + stemming\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, min_df = 2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_2)\n",
    "X_test_vec = vectorizer.transform(X_test_2)\n",
    "liste_donnees.append([X_train_vec,y_train_2,X_test_vec,y_test_2,len(vectorizer.get_feature_names()), \"Prétraitements basiques + stopwords + stemming + data augmentation : \"])\n",
    "\n",
    "# Prétraitement avec data augmentation (+ pré-traitements basiques + stop words + stemming + bigram et trigram au niv mot)\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, ngram_range=(1,3), analyzer='word', min_df = 2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_2)\n",
    "X_test_vec = vectorizer.transform(X_test_2)\n",
    "liste_donnees.append([X_train_vec,y_train_2,X_test_vec,y_test_2,len(vectorizer.get_feature_names()), \"Prétraitements basiques + stopwords + stemming + data augmentation + bigram/trigram niveau mot : \"])\n",
    "\n",
    "# Prétraitement avec data augmentation (+ pré-traitements basiques + stop words + stemming + bigram et trigram au niv caractère)\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, ngram_range=(2,4), analyzer='char', min_df = 2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_2)\n",
    "X_test_vec = vectorizer.transform(X_test_2)\n",
    "liste_donnees.append([X_train_vec,y_train_2,X_test_vec,y_test_2,len(vectorizer.get_feature_names()), \"Prétraitements basiques + stopwords + stemming + data augmentation + bigram/trigram niveau carac : \"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'étiquettes 1 - y_train_1:  5243  ( 0.13 )\n",
      "Nombre d'étiquettes -1 - y_train_1:  34946  ( 0.87 )\n",
      "Nombre d'étiquettes 1 - y_test_1:  2280  ( 0.13 )\n",
      "Nombre d'étiquettes -1 - y_test_1:  14944  ( 0.87 )\n"
     ]
    }
   ],
   "source": [
    "# Vérification que la classe minoritaire est bien étiquetée +1\n",
    "print(\"Nombre d'étiquettes 1 - y_train_1: \", liste_donnees[0][1].count(1), \" (\",round( liste_donnees[0][1].count(1)/len( liste_donnees[0][1]),2),\")\")\n",
    "print(\"Nombre d'étiquettes -1 - y_train_1: \",  liste_donnees[0][1].count(-1), \" (\",round( liste_donnees[0][1].count(-1)/len( liste_donnees[0][1]),2),\")\")\n",
    "\n",
    "print(\"Nombre d'étiquettes 1 - y_test_1: \", liste_donnees[0][3].count(1), \" (\",round( liste_donnees[0][3].count(1)/len( liste_donnees[0][3]),2),\")\")\n",
    "print(\"Nombre d'étiquettes -1 - y_test_1: \",  liste_donnees[0][3].count(-1), \" (\",round( liste_donnees[0][3].count(-1)/len( liste_donnees[0][3]),2),\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'étiquettes 1 - y_train :  31458  ( 0.47 )\n",
      "Nombre d'étiquettes -1 - y_train :  34946  ( 0.53 )\n"
     ]
    }
   ],
   "source": [
    "# Vérification du rééquilibrage des classes par data augmentation pour les 3 derniers jeux de données\n",
    "print(\"Nombre d'étiquettes 1 - y_train : \", y_train_2.count(1), \" (\",round(y_train_2.count(1)/len(y_train_2),2),\")\")\n",
    "print(\"Nombre d'étiquettes -1 - y_train : \", y_train_2.count(-1), \" (\",round(y_train_2.count(-1)/len(y_train_2),2),\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prétraitements basiques + stopwords + stemming : \n",
      "Taille vocab :  10028\n",
      "Prétraitements basiques + stopwords + stemming + bigram/trigram niveau mot : \n",
      "Taille vocab :  89909\n",
      "Prétraitements basiques + stopwords + stemming + bigram/trigram niveau caractère: \n",
      "Taille vocab :  25213\n",
      "Prétraitements basiques + stopwords + stemming + data augmentation : \n",
      "Taille vocab :  11402\n",
      "Prétraitements basiques + stopwords + stemming + data augmentation + bigram/trigram niveau mot : \n",
      "Taille vocab :  195226\n",
      "Prétraitements basiques + stopwords + stemming + data augmentation + bigram/trigram niveau carac : \n",
      "Taille vocab :  26334\n"
     ]
    }
   ],
   "source": [
    "# Taille du vocabulaire des données issues des différents pré-traitements\n",
    "for liste in liste_donnees:\n",
    "    print(liste[5])\n",
    "    print(\"Taille vocab : \",liste[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effets des différents prétraitements sur les performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeles = []\n",
    "\n",
    "compteur = 0\n",
    "for liste in liste_donnees: # remplacer liste_donnees par [liste_donnees[i]] pour tester un des jeux de données prétraité en particulier sinon ça prend beaucoup de temps\n",
    "    compteur += 1\n",
    "\n",
    "    # Pour chaque jeu de données prétraitées, on fit 3 classifieurs\n",
    "    # Pour chaque classifieur, on optimise ses paramètres en train avec gridSearch et on regarde les perf du meilleur estimateur en test\n",
    "\n",
    "    # Cas où on pénalise la classe minoritaire pour traiter le cas déséquilibré\n",
    "    if compteur == 1 or compteur == 2 or compteur == 3 :\n",
    "\n",
    "        ## Linear SVM\n",
    "        print(\"entree SVM\")\n",
    "        clf_svm = svm.LinearSVC(random_state=0, tol=1e-5, max_iter = 10000, class_weight = 'balanced')\n",
    "        param_grid_svm = {'C': [0.1, 1, 10, 100]}\n",
    "        grid_svm = GridSearchCV(clf_svm, param_grid = param_grid_svm, scoring = 'f1', n_jobs=3)\n",
    "        grid_svm.fit(liste[0],liste[1])\n",
    "\n",
    "        ## logistic regression\n",
    "        print(\"entree Reg Log\")\n",
    "        clf_log = LogisticRegression(max_iter = 10000, class_weight = 'balanced')\n",
    "        param_grid_log = {'C': [0.1, 1, 10, 100]}\n",
    "        grid_log = GridSearchCV(clf_log, param_grid = param_grid_log, scoring = 'f1', n_jobs=3)\n",
    "        grid_log.fit(liste[0],liste[1])\n",
    "    \n",
    "    # Cas où on a fait de la data augmentation en pré-traitement pour traiter le cas déséquilibré\n",
    "    else:\n",
    "        # Linear SVM \n",
    "        print(\"entree SVM\")\n",
    "        clf_svm = svm.LinearSVC(random_state=0, tol=1e-5, max_iter = 10000)\n",
    "        param_grid_svm = {'C': [0.1, 1, 10, 100]}\n",
    "        grid_svm = GridSearchCV(clf_svm, param_grid = param_grid_svm, scoring='f1', n_jobs=3)\n",
    "        grid_svm.fit(liste[0],liste[1])\n",
    "\n",
    "        ## Naive Bayes\n",
    "        print(\"entree NB\")\n",
    "        clf_nb = nb.MultinomialNB()\n",
    "        param_grid_nb = {'alpha': [0.0001, 0.001, 0.1, 1, 10, 100,1000]}\n",
    "        grid_nb = GridSearchCV(clf_nb, param_grid = param_grid_nb, scoring='f1', n_jobs=3)\n",
    "        grid_nb.fit(liste[0],liste[1])\n",
    "\n",
    "        # Logistic regression\n",
    "        print(\"entree Reg Log\")\n",
    "        clf_log = LogisticRegression(max_iter = 10000)\n",
    "        param_grid_log = {'C': [0.1, 1, 10, 100]}\n",
    "        grid_log = GridSearchCV(clf_log, param_grid = param_grid_log, scoring='f1', n_jobs=3)\n",
    "        grid_log.fit(liste[0],liste[1])\n",
    "\n",
    "    print(\" \")\n",
    "    print(liste[5])\n",
    "\n",
    "    print(\"taille vocabulaire : \",liste[4])\n",
    "    \n",
    "    print(\"SVM - paramètre C optimal :\", grid_svm.best_params_)\n",
    "    print(\"SVM - score f1  (avec C optimal) : \",f1_score(grid_svm.best_estimator_.predict(liste[0]),liste[1]))\n",
    "    color = 'white'\n",
    "    matrix = plot_confusion_matrix(grid_svm.best_estimator_, liste[0], liste[1], cmap=plt.cm.Blues, normalize = 'true')\n",
    "    matrix.ax_.set_title('SVM (score f1 : {})'.format(round(f1_score(grid_svm.best_estimator_.predict(liste[0]),liste[1]),2)), color=color)\n",
    "    plt.xlabel('Label prédit', color=color)\n",
    "    plt.ylabel('Vrai Label', color=color)\n",
    "    plt.gcf().axes[0].tick_params(colors=color)\n",
    "    plt.gcf().axes[1].tick_params(colors=color)\n",
    "    plt.show()\n",
    "\n",
    "    if compteur == 4 or compteur == 5 or compteur == 6 :\n",
    "        print(\"Naive Bayes - paramètre alpha optimal :\", grid_nb.best_params_)\n",
    "        print(\"Naive Bayes - score f1  (avec alpha optimal) : \",f1_score(grid_nb.best_estimator_.predict(liste[0]),liste[1]))\n",
    "        color = 'white'\n",
    "        matrix = plot_confusion_matrix(grid_nb.best_estimator_, liste[0], liste[1], cmap=plt.cm.Blues, normalize = 'true')\n",
    "        matrix.ax_.set_title('SVM (score f1 : {})'.format(round(f1_score(grid_nb.best_estimator_.predict(liste[0]),liste[1]),2)), color=color)\n",
    "        plt.xlabel('Label prédit', color=color)\n",
    "        plt.ylabel('Vrai Label', color=color)\n",
    "        plt.gcf().axes[0].tick_params(colors=color)\n",
    "        plt.gcf().axes[1].tick_params(colors=color)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Reg logisti - paramètre C optimal :\", grid_log.best_params_)\n",
    "    print(\"Reg logisti - score f1  (avec C optimal) : \",f1_score(grid_log.best_estimator_.predict(liste[0]),liste[1]))\n",
    "    color = 'white'\n",
    "    matrix = plot_confusion_matrix(grid_log.best_estimator_, liste[0], liste[1], cmap=plt.cm.Blues, normalize = 'true')\n",
    "    matrix.ax_.set_title('SVM (score f1 : {})'.format(round(f1_score(grid_log.best_estimator_.predict(liste[0]),liste[1]),2)), color=color)\n",
    "    plt.xlabel('Label prédit', color=color)\n",
    "    plt.ylabel('Vrai Label', color=color)\n",
    "    plt.gcf().axes[0].tick_params(colors=color)\n",
    "    plt.gcf().axes[1].tick_params(colors=color)\n",
    "    plt.show()\n",
    "\n",
    "    modeles.append([grid_svm.best_estimator_, \"Best SVM\",liste[2],liste[3]])\n",
    "    if compteur == 4 or compteur == 5 or compteur == 6 :\n",
    "        modeles.append([grid_nb.best_estimator_, \"Best NB\",liste[2],liste[3]])\n",
    "    modeles.append([grid_log.best_estimator_, \"Best Reg Log\",liste[2],liste[3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score f1 :  0.5132394366197184\n",
      "[[14585  1369]\n",
      " [  359   911]]\n",
      "[[0.9141908  0.0858092 ]\n",
      " [0.28267717 0.71732283]]\n"
     ]
    }
   ],
   "source": [
    "# On construit le modèle final correspondant au vote de plusieurs modèles\n",
    "# Et on calcule ses performances en test\n",
    "y_pred = []\n",
    "y_moy = []\n",
    "modeles_selectionnees = [modeles[2],modeles[3],modeles[9],modeles[10],modeles[11]] # Indices correspondant aux modèles sélectionnés pour le vote\n",
    "\n",
    "for mod in modeles_selectionnees: \n",
    "    y_pred.append(mod[0].predict(mod[2]))\n",
    "\n",
    "for i in range(0,len(y_pred[0])):\n",
    "    if [y[i] for y in y_pred].count(1) > len(modeles_selectionnees)//2: # résultat du \"vote\" des modèles\n",
    "        y_moy.append(1)\n",
    "    else:\n",
    "        y_moy.append(-1)\n",
    "\n",
    "conf_matrix_0 = confusion_matrix(y_moy, y_test)\n",
    "conf_matrix = confusion_matrix(y_moy, y_test, normalize = 'true')\n",
    "print(conf_matrix_0)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération du fichier .txt de résultats\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Phrase': X_test, 'Classe réelle': y_test, 'Classe prédite': y_moy}, columns = ['Phrase', 'Classe réelle', 'Classe prédite'])\n",
    "df.to_csv(\"fichier.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
