{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import re\n",
    "import os.path\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import svm, linear_model\n",
    "from sklearn.metrics import f1_score, confusion_matrix, plot_confusion_matrix, average_precision_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données:\n",
    "def load_movies(path2data): # 1 classe par répertoire\n",
    "    alltxts = [] # init vide\n",
    "    labs = []\n",
    "    cpt = 0\n",
    "    for cl in os.listdir(path2data): # parcours des fichiers d'un répertoire\n",
    "        for f in os.listdir(path2data+cl):\n",
    "            txt = open(path2data+cl+'/'+f).read()\n",
    "            alltxts.append(txt)\n",
    "            labs.append(cpt)\n",
    "        cpt+=1 # chg répertoire = cht classe\n",
    "        \n",
    "    return alltxts,labs\n",
    "\n",
    "path = \"/Users/antoinedegenne/Desktop/PAPT/3A/Web_semantique/Vincent/tuto_TAL-main-3/notebooks/ressources/movies/movies1000/\"\n",
    "alltxts,alllabs = load_movies(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de phrase :  2000\n",
      "Nombre d'étiquettes 1 :  1000  ( 0.5 )\n",
      "Nombre d'étiquettes 0 :  1000  ( 0.5 )\n"
     ]
    }
   ],
   "source": [
    "# Nombre de phrase\n",
    "print(\"Nombre de phrase : \", len(alltxts))\n",
    "\n",
    "# Mise en évidénce du cas déséquilibré\n",
    "print(\"Nombre d'étiquettes 1 : \", alllabs.count(1), \" (\",round(alllabs.count(1)/len(alllabs),2),\")\")\n",
    "print(\"Nombre d'étiquettes 0 : \", alllabs.count(0), \" (\",round(alllabs.count(0)/len(alllabs),2),\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de prétraitements basiques\n",
    "\n",
    "# Supprimer la ponctuation et les chiffres de toutes les phrases\n",
    "def suppr_ponct(txt): # prend alltxts en entrée\n",
    "    punc = string.punctuation  \n",
    "    punc += '\\n\\r\\t'\n",
    "\n",
    "    for i in range(len(txt)):\n",
    "        txt[i] = txt[i].translate(str.maketrans(punc, ' ' * len(punc)))  \n",
    "        txt[i] = re.sub('[0-9]+', '', txt[i])\n",
    "\n",
    "# Supprimer les accents et caractères spéciaux\n",
    "def suppr_accents_maj(txt): # prend alltxts en entrée\n",
    "    for i in range(len(txt)):\n",
    "        txt[i] = unicodedata.normalize('NFD',  txt[i]).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "        txt[i] =  txt[i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Génération du dictionnaire {\"mot\" : occurence_documentaire}\n",
    "suppr_ponct(alltxts)\n",
    "suppr_accents_maj(alltxts)\n",
    "\n",
    "words = []\n",
    "for i in range(len(alltxts)):\n",
    "    mots_vus = []\n",
    "    for mot in alltxts[i].split():\n",
    "        if mot not in mots_vus: # on ajoute 1 seule fois le mot s'il apparait dans la phrase --> occurence documentaire et pas totale\n",
    "            words.append(mot)\n",
    "            mots_vus.append(mot)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "dico = Counter(words)\n",
    "\n",
    "# Dico qui associe 1 mot à sa position dans le dictionnaire dico\n",
    "trans = dict(zip(list(dico.keys()), np.arange(len(dico)).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction du vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du vocabulaire :  38890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoinedegenne/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Extraction du vocabulaire\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(alltxts)\n",
    "\n",
    "print(\"Taille du vocabulaire : \",len(vectorizer.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 mots les plus apparus :  [('the', 1999), ('of', 1998), ('and', 1998), ('to', 1997), ('a', 1996), ('is', 1995), ('in', 1994), ('it', 1967), ('s', 1966), ('that', 1963), ('with', 1944), ('for', 1922), ('as', 1920), ('but', 1903), ('this', 1896), ('on', 1861), ('an', 1785), ('by', 1782), ('are', 1781), ('one', 1776), ('be', 1774), ('his', 1756), ('film', 1753), ('who', 1749), ('at', 1720), ('from', 1702), ('t', 1688), ('he', 1686), ('not', 1681), ('have', 1631), ('has', 1606), ('all', 1603), ('i', 1576), ('movie', 1554), ('out', 1539), ('like', 1497), ('there', 1496), ('was', 1496), ('they', 1495), ('so', 1484), ('you', 1474), ('more', 1469), ('up', 1461), ('about', 1461), ('when', 1449), ('what', 1399), ('or', 1388), ('can', 1368), ('some', 1365), ('if', 1350), ('just', 1327), ('which', 1320), ('into', 1310), ('only', 1309), ('their', 1295), ('even', 1291), ('than', 1284), ('time', 1258), ('no', 1206), ('most', 1186), ('good', 1182), ('much', 1138), ('would', 1136), ('him', 1130), ('her', 1116), ('other', 1108), ('we', 1106), ('get', 1100), ('been', 1088), ('do', 1080), ('well', 1080), ('story', 1078), ('its', 1076), ('will', 1072), ('also', 1071), ('after', 1068), ('two', 1063), ('first', 1047), ('character', 1047), ('them', 1038), ('make', 1017), ('way', 1016), ('see', 974), ('off', 970), ('very', 967), ('characters', 965), ('does', 964), ('she', 959), ('while', 958), ('any', 956), ('too', 948), ('where', 931), ('over', 929), ('little', 921), ('because', 916), ('how', 913), ('had', 900), ('director', 898), ('plot', 892), ('could', 890)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc09be66580>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Affichage des 100 mots les plus apparus dans les phrases\n",
    "print(\"100 mots les plus apparus : \",dico.most_common(100))\n",
    "freq = [f for w,f in dico.most_common()]\n",
    "\n",
    "plt.plot(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation paramétrique du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINITION DES FONCTIONS DE PRETRAITEMENT\n",
    "\n",
    "# Data augmentation pour rééquilibrer les classes\n",
    "def data_augmentation(txt,labs):\n",
    "    liste_positions = []\n",
    "    for i in labs:\n",
    "        liste_positions.append(i == 1) \n",
    "\n",
    "    liste_classe_minoritaire = itertools.compress(txt, liste_positions)\n",
    "\n",
    "    txts_bis = []\n",
    "    for each in liste_classe_minoritaire:\n",
    "        txts_bis.append(each)\n",
    "\n",
    "    for elem in txts_bis: # nombre d'ajout trouvé de manière empirique pour avoir environ 50/50\n",
    "        txt.append(elem)\n",
    "        txt.append(elem)\n",
    "        txt.append(elem)\n",
    "        txt.append(elem)\n",
    "        txt.append(elem)\n",
    "        labs.append(1)\n",
    "        labs.append(1)\n",
    "        labs.append(1)\n",
    "        labs.append(1)\n",
    "        labs.append(1)\n",
    "\n",
    "# Stemming\n",
    "def stemming(txt):\n",
    "    ps = PorterStemmer()\n",
    "    compteur = 0\n",
    "    for sentence in txt:\n",
    "        # print(\"sentence : \",sentence)\n",
    "        words = word_tokenize(sentence,language='french')\n",
    "        # print(\"words : \",words)\n",
    "        sentence_new = ''\n",
    "        for word in words:\n",
    "            word_new = ps.stem(word)\n",
    "            sentence_new += word_new + ' '\n",
    "        txt[compteur] = sentence_new\n",
    "        compteur += 1\n",
    "\n",
    "# Stopword\n",
    "stop_words = stopwords.words('french')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISE EN PLACE DES PRETRAITEMENTS --> GENERATION DE DIFFERENTS JEUX DE DONNEES\n",
    "# 3 prétraitrements différents : prétaitements basiques + stop words + stemming, n-gram niveau mots, n-gram niveau caractères\n",
    "\n",
    "# Séparation train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(alltxts, alllabs, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "# On ajoute à liste_donnees différents jeux de données issus de différents pré-traitements\n",
    "liste_donnees = []\n",
    "\n",
    "# Prétraitement par stemming commun à tous les jeux de données\n",
    "stemming(X_train)\n",
    "stemming(X_test)\n",
    "\n",
    "#######################################\n",
    "# PRETRAITEMENTS SANS DATA AUGMENTATION\n",
    "#######################################\n",
    "X_train_1 = X_train.copy()\n",
    "X_test_1 = X_test.copy()\n",
    "y_train_1 = y_train.copy()\n",
    "y_test_1 = y_test.copy()\n",
    "\n",
    "# Prétraitement avec pré-traitements basiques + stop words + stemming\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, min_df = 2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_1)\n",
    "X_test_vec = vectorizer.transform(X_test_1)\n",
    "liste_donnees.append([X_train_vec,y_train_1,X_test_vec,y_test_1,len(vectorizer.get_feature_names()), \"Prétraitements basiques + stopwords + stemming : \"])\n",
    "\n",
    "# Prétraitement avec bigram et trigram au niv mot (+ pré-traitements basiques + stop words + stemming)\n",
    "vectorizer = CountVectorizer(stop_words = stop_words, ngram_range=(1,3), analyzer='word', min_df = 2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_1)\n",
    "X_test_vec = vectorizer.transform(X_test_1)\n",
    "liste_donnees.append([X_train_vec,y_train_1,X_test_vec,y_test_1,len(vectorizer.get_feature_names()), \"Prétraitements basiques + stopwords + stemming + bigram/trigram niveau mot : \"])\n",
    "\n",
    "# Prétraitement avec bigram et trigram au niv caractère (+ pré-traitements basiques + stop words + stemming)\n",
    "vectorizer = CountVectorizer(stop_words = stop_words,ngram_range=(2,4), analyzer='char', min_df = 2)\n",
    "X_train_vec = vectorizer.fit_transform(X_train_1)\n",
    "X_test_vec = vectorizer.transform(X_test_1)\n",
    "liste_donnees.append([X_train_vec,y_train_1,X_test_vec,y_test_1,len(vectorizer.get_feature_names()), \"Prétraitements basiques + stopwords + stemming + bigram/trigram niveau caractère: \"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prétraitements basiques + stopwords + stemming : \n",
      "Taille vocab :  13817\n",
      "Prétraitements basiques + stopwords + stemming + bigram/trigram niveau mot : \n",
      "Taille vocab :  155194\n",
      "Prétraitements basiques + stopwords + stemming + bigram/trigram niveau caractère: \n",
      "Taille vocab :  40637\n"
     ]
    }
   ],
   "source": [
    "# Taille du vocabulaire des données issues des différents pré-traitements\n",
    "for liste in liste_donnees:\n",
    "    print(liste[5])\n",
    "    print(\"Taille vocab : \",liste[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effets des différents prétraitements sur les performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "entree SVM\n",
      "entree Reg Log\n",
      "entree Naives Bayes\n",
      " \n",
      "Prétraitements basiques + stopwords + stemming : \n",
      "taille vocabulaire :  13817\n",
      "SVM - paramètre C optimal : {'C': 0.001}\n",
      "SVM - Perf en apprentissage (avec C optimal) :  0.9907142857142858\n",
      "Reg logisti - paramètre C optimal : {'C': 0.01}\n",
      "Reg logisti - Perf en apprentissage  (avec C optimal) :  0.9871428571428571\n",
      "Naives Bayes - paramètre alpha optimal : {'alpha': 0.8}\n",
      "Naives Bayes - Perf en apprentissage  (avec alpha optimal) :  0.9571428571428572\n",
      "\n",
      "entree SVM\n",
      "entree Reg Log\n",
      "entree Naives Bayes\n",
      " \n",
      "Prétraitements basiques + stopwords + stemming + bigram/trigram niveau mot : \n",
      "taille vocabulaire :  155194\n",
      "SVM - paramètre C optimal : {'C': 0.001}\n",
      "SVM - Perf en apprentissage (avec C optimal) :  1.0\n",
      "Reg logisti - paramètre C optimal : {'C': 0.1}\n",
      "Reg logisti - Perf en apprentissage  (avec C optimal) :  1.0\n",
      "Naives Bayes - paramètre alpha optimal : {'alpha': 0.5}\n",
      "Naives Bayes - Perf en apprentissage  (avec alpha optimal) :  1.0\n",
      "\n",
      "entree SVM\n",
      "entree Reg Log\n",
      "entree Naives Bayes\n",
      " \n",
      "Prétraitements basiques + stopwords + stemming + bigram/trigram niveau caractère: \n",
      "taille vocabulaire :  40637\n",
      "SVM - paramètre C optimal : {'C': 0.0001}\n",
      "SVM - Perf en apprentissage (avec C optimal) :  0.9935714285714285\n",
      "Reg logisti - paramètre C optimal : {'C': 0.001}\n",
      "Reg logisti - Perf en apprentissage  (avec C optimal) :  0.9935714285714285\n",
      "Naives Bayes - paramètre alpha optimal : {'alpha': 0.5}\n",
      "Naives Bayes - Perf en apprentissage  (avec alpha optimal) :  0.9271428571428572\n"
     ]
    }
   ],
   "source": [
    "modeles = []\n",
    "\n",
    "compteur = 0\n",
    "for liste in liste_donnees: # remplacer liste_donnees par [liste_donnees[i]] pour tester un des jeux de données prétraité en particulier sinon ça prend beaucoup de temps\n",
    "    compteur += 1\n",
    "\n",
    "    # Pour chaque jeu de données prétraitées, on fit 3 classifieurs\n",
    "    # Pour chaque classifieur, on optimise ses paramètres en train avec gridSearch et on regarde les perf du meilleur estimateur en test\n",
    "    ## Linear SVM\n",
    "    print(\"\\nentree SVM\")\n",
    "    clf_svm = svm.LinearSVC(random_state=0, tol=1e-5, max_iter = 10000)\n",
    "    param_grid_svm = {'C': [0.0001, 0.001, 0.01, 0.1]}\n",
    "    grid_svm = GridSearchCV(clf_svm, param_grid = param_grid_svm, n_jobs=3)\n",
    "    grid_svm.fit(liste[0],liste[1])\n",
    "\n",
    "    ## logistic regression\n",
    "    print(\"entree Reg Log\")\n",
    "    clf_log = LogisticRegression(max_iter = 10000)\n",
    "    param_grid_log = {'C': [0.0001, 0.001, 0.01, 0.1]}\n",
    "    grid_log = GridSearchCV(clf_log, param_grid = param_grid_log, n_jobs=3)\n",
    "    grid_log.fit(liste[0],liste[1])\n",
    "\n",
    "    ## Naives Bayes\n",
    "    print(\"entree Naives Bayes\")\n",
    "    clf_nvb = MultinomialNB()\n",
    "    param_grid_nvb = {'alpha': [0.1, 0.5, 0.8, 1]}\n",
    "    grid_nvb = GridSearchCV(clf_nvb, param_grid = param_grid_nvb, n_jobs=3)\n",
    "    grid_nvb.fit(liste[0],liste[1])\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\" \")\n",
    "    print(liste[5])\n",
    "    print(\"taille vocabulaire : \",liste[4])\n",
    "    \n",
    "    print(\"SVM - paramètre C optimal :\", grid_svm.best_params_)\n",
    "    print(\"SVM - Perf en apprentissage (avec C optimal) : \",grid_svm.best_estimator_.score(liste[0],liste[1]))\n",
    "    \n",
    "\n",
    "    print(\"Reg logisti - paramètre C optimal :\", grid_log.best_params_)\n",
    "    print(\"Reg logisti - Perf en apprentissage  (avec C optimal) : \",grid_log.best_estimator_.score(liste[0],liste[1]))\n",
    "\n",
    "\n",
    "    print(\"Naives Bayes - paramètre alpha optimal :\", grid_nvb.best_params_)\n",
    "    print(\"Naives Bayes - Perf en apprentissage  (avec alpha optimal) : \",grid_nvb.best_estimator_.score(liste[0],liste[1]))\n",
    "\n",
    "\n",
    "\n",
    "    modeles.append([grid_svm.best_estimator_, \"Best SVM\",liste[2],liste[3]])\n",
    "    modeles.append([grid_log.best_estimator_, \"Best Reg Log\",liste[2],liste[3]])\n",
    "    modeles.append([grid_nvb.best_estimator_, \"Best Naives Bayes\",liste[2],liste[3]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[252  48]\n",
      " [ 56 244]]\n",
      "[[0.84       0.16      ]\n",
      " [0.18666667 0.81333333]]\n"
     ]
    }
   ],
   "source": [
    "# On construit le modèle final correspondant au vote de plusieurs modèles\n",
    "# Et on calcule ses performances en test\n",
    "y_pred = []\n",
    "y_moy = []\n",
    "modeles_selectionnees = [modeles[3],modeles[4],modeles[5]] # Indices correspondant aux modèles sélectionnés pour le vote\n",
    "\n",
    "for mod in modeles_selectionnees: \n",
    "    y_pred.append(mod[0].predict(mod[2]))\n",
    "\n",
    "for i in range(0,len(y_pred[0])):\n",
    "    if [y[i] for y in y_pred].count(1) > len(modeles_selectionnees)//2: # résultat du \"vote\" des modèles\n",
    "        y_moy.append(1)\n",
    "    else:\n",
    "        y_moy.append(0)\n",
    "\n",
    "conf_matrix_0 = confusion_matrix(y_moy, y_test)\n",
    "conf_matrix = confusion_matrix(y_moy, y_test, normalize = 'true')\n",
    "print(conf_matrix_0)\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Génération du fichier .txt de résultats\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'Phrase': X_test, 'Classe réelle': y_test, 'Classe prédite': y_moy}, columns = ['Phrase', 'Classe réelle', 'Classe prédite'])\n",
    "df.to_csv(\"fichier.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b232fc45960c9dbd0c0efb27ff96954c5713886543565220a28c70b42cf4b7a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
